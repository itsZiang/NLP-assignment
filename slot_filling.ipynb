{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport torch\nfrom transformers import BertTokenizerFast, BertForTokenClassification, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import classification_report\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:16:31.780855Z","iopub.execute_input":"2024-12-31T12:16:31.781205Z","iopub.status.idle":"2024-12-31T12:16:37.271057Z","shell.execute_reply.started":"2024-12-31T12:16:31.781176Z","shell.execute_reply":"2024-12-31T12:16:37.270349Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/itsZiang/data.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:16:37.272018Z","iopub.execute_input":"2024-12-31T12:16:37.272345Z","iopub.status.idle":"2024-12-31T12:16:40.183111Z","shell.execute_reply.started":"2024-12-31T12:16:37.272326Z","shell.execute_reply":"2024-12-31T12:16:40.182249Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'data'...\nremote: Enumerating objects: 16, done.\u001b[K\nremote: Counting objects: 100% (16/16), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 16 (delta 4), reused 6 (delta 2), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (16/16), 7.97 MiB | 8.10 MiB/s, done.\nResolving deltas: 100% (4/4), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"MAX_LEN = 64\nTRAIN_BATCH_SIZE = 128\nVALID_BATCH_SIZE = 32\nTEST_BATCH_SIZE = 32\nEPOCHS = 4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:16:40.185211Z","iopub.execute_input":"2024-12-31T12:16:40.185459Z","iopub.status.idle":"2024-12-31T12:16:40.189375Z","shell.execute_reply.started":"2024-12-31T12:16:40.185430Z","shell.execute_reply":"2024-12-31T12:16:40.188603Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load datasets\ndef load_data(filepath):\n    with open(filepath, 'r') as f:\n        return json.load(f)\n\n# Extract all unique labels from data\ndef extract_labels(datasets):\n    labels = set()\n    for data in datasets:\n        for item in data:\n            for tag, slot, _, _, _ in item['span_info']:\n                labels.add(f\"B-{tag}:{slot}\")\n                labels.add(f\"I-{tag}:{slot}\")\n    labels.add(\"O\")\n    return sorted(labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:16:40.190611Z","iopub.execute_input":"2024-12-31T12:16:40.190926Z","iopub.status.idle":"2024-12-31T12:16:40.205644Z","shell.execute_reply.started":"2024-12-31T12:16:40.190878Z","shell.execute_reply":"2024-12-31T12:16:40.204981Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Dataset class\nclass SlotFillingDataset(Dataset):\n    def __init__(self, data, tokenizer, label2id, max_len=MAX_LEN):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.label2id = label2id\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        utterance = self.data[idx]['utterance']\n        span_info = self.data[idx]['span_info']\n\n        # Tokenize utterance\n        tokens = self.tokenizer(utterance, truncation=True, padding='max_length', max_length=self.max_len, return_offsets_mapping=True)\n        labels = ['O'] * len(tokens['input_ids'])\n\n        for tag, slot, value, start, end in span_info:\n            bio_tag = f\"{tag}:{slot}\"\n            for i, (offset_start, offset_end) in enumerate(tokens['offset_mapping']):\n                if offset_start == start:\n                    labels[i] = f\"B-{bio_tag}\"\n                elif offset_start > start and offset_end <= end:\n                    labels[i] = f\"I-{bio_tag}\"\n\n        labels = [self.label2id[label] for label in labels]\n\n        return {\n            'input_ids': torch.tensor(tokens['input_ids']),\n            'attention_mask': torch.tensor(tokens['attention_mask']),\n            'labels': torch.tensor(labels)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:16:40.206494Z","iopub.execute_input":"2024-12-31T12:16:40.206718Z","iopub.status.idle":"2024-12-31T12:16:40.220836Z","shell.execute_reply.started":"2024-12-31T12:16:40.206699Z","shell.execute_reply":"2024-12-31T12:16:40.220206Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Define training function\ndef train_model(model, train_loader, val_loader, optimizer, device, num_epochs=5):\n    best_val_loss = float('inf')\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n        model.train()\n        total_loss = 0\n\n        # Training loop with tqdm\n        train_progress = tqdm(train_loader, desc=\"Training\", leave=False)\n        for batch in train_progress:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            train_progress.set_postfix(loss=loss.item())\n\n        avg_train_loss = total_loss / len(train_loader)\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            val_progress = tqdm(val_loader, desc=\"Validating\", leave=False)\n            for batch in val_progress:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n\n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                val_loss += outputs.loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)\n        print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save(model.state_dict(), '/kaggle/working/best_model.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:16:40.221576Z","iopub.execute_input":"2024-12-31T12:16:40.221762Z","iopub.status.idle":"2024-12-31T12:16:40.237553Z","shell.execute_reply.started":"2024-12-31T12:16:40.221745Z","shell.execute_reply":"2024-12-31T12:16:40.236957Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load and process data\ndata_train = load_data('/kaggle/working/data/data_slot_filling_train.json')\ndata_dev = load_data('/kaggle/working/data/data_slot_filling_dev.json')\ndata_test = load_data('/kaggle/working/data/data_slot_filling_test.json')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:16:40.238293Z","iopub.execute_input":"2024-12-31T12:16:40.238516Z","iopub.status.idle":"2024-12-31T12:16:41.178956Z","shell.execute_reply.started":"2024-12-31T12:16:40.238486Z","shell.execute_reply":"2024-12-31T12:16:41.177976Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Extract labels from all datasets\nall_labels = extract_labels([data_train, data_dev, data_test])\nlabel2id = {label: idx for idx, label in enumerate(all_labels)}\nid2label = {idx: label for label, idx in label2id.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:16:41.180845Z","iopub.execute_input":"2024-12-31T12:16:41.181184Z","iopub.status.idle":"2024-12-31T12:16:41.265062Z","shell.execute_reply.started":"2024-12-31T12:16:41.181160Z","shell.execute_reply":"2024-12-31T12:16:41.264167Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\ntrain_dataset = SlotFillingDataset(data_train, tokenizer, label2id)\ndev_dataset = SlotFillingDataset(data_dev, tokenizer, label2id)\ntest_dataset = SlotFillingDataset(data_test, tokenizer, label2id)\n\ntrain_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_dataset, batch_size=VALID_BATCH_SIZE)\ntest_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:16:41.266163Z","iopub.execute_input":"2024-12-31T12:16:41.266485Z","iopub.status.idle":"2024-12-31T12:16:42.800343Z","shell.execute_reply.started":"2024-12-31T12:16:41.266450Z","shell.execute_reply":"2024-12-31T12:16:42.799474Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee79d8515b23464cbcc501a406f4ca77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb47c530ec0c48b09f33011f5d38f9f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21d195fc1a56462ba8bac16ac0ef91a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d55765766da4e8da2fcfacb7ef57b11"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Model and optimizer\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(all_labels))\nmodel.to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:16:42.801253Z","iopub.execute_input":"2024-12-31T12:16:42.801584Z","iopub.status.idle":"2024-12-31T12:16:46.648787Z","shell.execute_reply.started":"2024-12-31T12:16:42.801550Z","shell.execute_reply":"2024-12-31T12:16:46.647937Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97024ddd12d24749a552aae3c77b39b8"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Train model\ntrain_model(model, train_loader, dev_loader, optimizer, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:16:46.649634Z","iopub.execute_input":"2024-12-31T12:16:46.650065Z","iopub.status.idle":"2024-12-31T12:49:44.069217Z","shell.execute_reply.started":"2024-12-31T12:16:46.650031Z","shell.execute_reply":"2024-12-31T12:49:44.068360Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4477, Val Loss: 0.1592\nEpoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1441, Val Loss: 0.1149\nEpoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1071, Val Loss: 0.1009\nEpoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0897, Val Loss: 0.1063\nEpoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"                                                                        ","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0790, Val Loss: 0.1013\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load('/kaggle/working/best_model.pt'))\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T12:51:59.145858Z","iopub.execute_input":"2024-12-31T12:51:59.146319Z","iopub.status.idle":"2024-12-31T12:51:59.488538Z","shell.execute_reply.started":"2024-12-31T12:51:59.146284Z","shell.execute_reply":"2024-12-31T12:51:59.487551Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-14-20dfa41fabe7>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/best_model.pt'))\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"BertForTokenClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=365, bias=True)\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Predict example utterance\nutterance = \"I've booked that. Your reference number is 0ICM79OM .\"\ntokens = tokenizer(utterance, return_tensors=\"pt\", truncation=True, padding=True, return_offsets_mapping=True)\noffsets = tokens.pop(\"offset_mapping\")[0]\ninput_ids = tokens['input_ids'].to(device)\nattention_mask = tokens['attention_mask'].to(device)\n\noutputs = model(input_ids, attention_mask=attention_mask)\nlogits = outputs.logits\npredicted_labels = torch.argmax(logits, dim=-1).squeeze().tolist()\npredicted_tags = [id2label[label] for label in predicted_labels]\n\n# Map tokens to slot-value pairs\ndecoded_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\nslot_value_pairs = []\ncurrent_slot = None\ncurrent_value = \"\"\n\nfor token, tag, offset in zip(decoded_tokens, predicted_tags, offsets):\n    if tag.startswith(\"B-\"):\n        if current_slot is not None and current_value:\n            slot_value_pairs.append((current_slot, current_value.strip()))\n        current_slot = tag[2:]\n        current_value = tokenizer.convert_tokens_to_string([token])\n    elif tag.startswith(\"I-\") and current_slot == tag[2:]:\n        current_value += tokenizer.convert_tokens_to_string([token])\n    else:\n        if current_slot is not None and current_value:\n            slot_value_pairs.append((current_slot, current_value.strip()))\n        current_slot = None\n        current_value = \"\"\n\nif current_slot is not None and current_value:\n    slot_value_pairs.append((current_slot, current_value.strip()))\n\nprint(f\"Tokens: {decoded_tokens}\")\nprint(f\"Predicted Tags: {predicted_tags}\")\nprint(f\"Slot-Value Pairs: {slot_value_pairs}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T13:24:53.452465Z","iopub.execute_input":"2024-12-31T13:24:53.452775Z","iopub.status.idle":"2024-12-31T13:24:53.473722Z","shell.execute_reply.started":"2024-12-31T13:24:53.452750Z","shell.execute_reply":"2024-12-31T13:24:53.473069Z"}},"outputs":[{"name":"stdout","text":"Tokens: ['[CLS]', 'their', 'phone', 'number', 'is', '01', '##22', '##33', '##53', '##11', '##0', '.', 'can', 'i', 'help', 'you', 'with', 'anything', 'else', 'today', '[SEP]']\nPredicted Tags: ['O', 'O', 'O', 'O', 'O', 'B-Restaurant-Inform:phone', 'I-Attraction-Inform:phone', 'I-Attraction-Inform:phone', 'I-Attraction-Inform:phone', 'I-Attraction-Inform:phone', 'I-Attraction-Inform:phone', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\nSlot-Value Pairs: [('Restaurant-Inform:phone', '01')]\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"def predict_slot_values(text):\n    # Tokenize input text\n    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, return_offsets_mapping=True)\n    offsets = tokens.pop(\"offset_mapping\")[0]\n    input_ids = tokens['input_ids'].to(device)\n    attention_mask = tokens['attention_mask'].to(device)\n\n    # Get model predictions\n    outputs = model(input_ids, attention_mask=attention_mask)\n    logits = outputs.logits\n    predicted_labels = torch.argmax(logits, dim=-1).squeeze().tolist()\n    predicted_tags = [id2label[label] for label in predicted_labels]\n\n    # Map tokens to slot-value pairs\n    decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n    slot_value_pairs = []\n    current_slot = None\n    current_value_tokens = []\n    current_start = None\n\n    for token, tag, offset in zip(decoded_tokens, predicted_tags, offsets):\n        if token in [\"[CLS]\", \"[SEP]\"]:  # Skip special tokens\n            continue\n\n        if tag.startswith(\"B-\"):\n            # Save the previous slot-value pair\n            if current_slot is not None and current_value_tokens:\n                value = text[current_start:offset[0]].strip()\n                slot_value_pairs.append((current_slot, value))\n            # Start a new slot-value pair\n            current_slot = tag[2:]\n            current_value_tokens = [token.replace(\"##\", \"\")]\n            current_start = offset[0]\n        elif tag.startswith(\"I-\") and current_slot == tag[2:]:\n            # Append to the current slot-value pair\n            current_value_tokens.append(token.replace(\"##\", \"\"))\n        else:\n            # Save the previous slot-value pair\n            if current_slot is not None and current_value_tokens:\n                value = text[current_start:offset[0]].strip()\n                slot_value_pairs.append((current_slot, value))\n            # Reset for the next slot\n            current_slot = None\n            current_value_tokens = []\n            current_start = None\n\n    # Save the last slot-value pair\n    if current_slot is not None and current_value_tokens:\n        value = text[current_start:].strip()\n        slot_value_pairs.append((current_slot, value))\n\n    return slot_value_pairs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T13:25:57.648431Z","iopub.execute_input":"2024-12-31T13:25:57.648722Z","iopub.status.idle":"2024-12-31T13:25:57.657335Z","shell.execute_reply.started":"2024-12-31T13:25:57.648698Z","shell.execute_reply":"2024-12-31T13:25:57.656461Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"text = \"Sure. There are several churches and an old schools attraction, all in the centre area. Do you have a preference?\"\nslot_value_pairs = predict_slot_values(text)\nfor slot, value in slot_value_pairs:\n    print(f'slot: {slot} / value: {value}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T13:55:13.650474Z","iopub.execute_input":"2024-12-31T13:55:13.650814Z","iopub.status.idle":"2024-12-31T13:55:13.668088Z","shell.execute_reply.started":"2024-12-31T13:55:13.650782Z","shell.execute_reply":"2024-12-31T13:55:13.667050Z"}},"outputs":[{"name":"stdout","text":"slot: Attraction-Inform:choice / value: several\nslot: Attraction-Inform:type / value: churches\nslot: Attraction-Inform:type / value: old schools\nslot: Attraction-Inform:area / value: centre\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"\ndef calculate_metrics_from_json(json_file_path, tokenizer, model, id2label, device):\n    utterance_metrics = []\n    entity_metrics = {'true_positive': 0, 'false_positive': 0, 'false_negative': 0}\n    total_utterances = 0\n    utterance_correct = 0  # To calculate utterance-level accuracy\n\n    # Load the JSON file\n    with open(json_file_path, 'r') as f:\n        test_data = json.load(f)\n    \n    total_utterances = len(test_data)\n\n    for sample in test_data:\n        # Ground truth entities\n        ground_truth = {(tag, slot, value) for tag, slot, value, _, _ in sample['span_info']}\n        \n        # Predicted entities\n        utterance = sample['utterance']\n        predicted_slot_value_pairs = predict_slot_values(utterance)\n        predicted = {(slot.split(':')[0], slot.split(':')[1], value) for slot, value in predicted_slot_value_pairs}\n        \n        # Entity-Level Metrics\n        true_positives = len(ground_truth & predicted)  # Intersection of ground truth and predicted\n        false_positives = len(predicted - ground_truth)  # Predicted but not in ground truth\n        false_negatives = len(ground_truth - predicted)  # Ground truth but not predicted\n\n        entity_metrics['true_positive'] += true_positives\n        entity_metrics['false_positive'] += false_positives\n        entity_metrics['false_negative'] += false_negatives\n\n        # Utterance-Level Metrics\n        utterance_correct += 1 if ground_truth == predicted else 0\n        precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n        recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n        f1 = (2 * precision * recall / (precision + recall)) if precision + recall > 0 else 0\n\n        utterance_metrics.append({'precision': precision, 'recall': recall, 'f1': f1})\n\n    # Aggregate Utterance-Level Metrics\n    avg_precision = sum(m['precision'] for m in utterance_metrics) / total_utterances\n    avg_recall = sum(m['recall'] for m in utterance_metrics) / total_utterances\n    avg_f1 = sum(m['f1'] for m in utterance_metrics) / total_utterances\n    utterance_accuracy = utterance_correct / total_utterances\n\n    # Aggregate Entity-Level Metrics\n    tp, fp, fn = entity_metrics['true_positive'], entity_metrics['false_positive'], entity_metrics['false_negative']\n    entity_precision = tp / (tp + fp) if tp + fp > 0 else 0\n    entity_recall = tp / (tp + fn) if tp + fn > 0 else 0\n    entity_f1 = (2 * entity_precision * entity_recall / (entity_precision + entity_recall)) if entity_precision + entity_recall > 0 else 0\n    entity_accuracy = tp / (tp + fp + fn) if tp + fp + fn > 0 else 0\n\n    # Print Results\n    print(\"Utterance-Level Metrics:\")\n    print(f\"Precision: {avg_precision:.2f}, Recall: {avg_recall:.2f}, F1 Score: {avg_f1:.2f}, Accuracy: {utterance_accuracy:.2f}\")\n    print(\"Entity-Level Metrics:\")\n    print(f\"Precision: {entity_precision:.2f}, Recall: {entity_recall:.2f}, F1 Score: {entity_f1:.2f}, Accuracy: {entity_accuracy:.2f}\")\n\n# Example usage:\ncalculate_metrics_from_json('/kaggle/working/data/data_slot_filling_test.json', tokenizer, model, id2label, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T14:01:46.270783Z","iopub.execute_input":"2024-12-31T14:01:46.271139Z","iopub.status.idle":"2024-12-31T14:03:02.859419Z","shell.execute_reply.started":"2024-12-31T14:01:46.271108Z","shell.execute_reply":"2024-12-31T14:03:02.858500Z"}},"outputs":[{"name":"stdout","text":"Utterance-Level Metrics:\nPrecision: 0.70, Recall: 0.70, F1 Score: 0.70, Accuracy: 0.54\nEntity-Level Metrics:\nPrecision: 0.72, Recall: 0.73, F1 Score: 0.72, Accuracy: 0.57\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"def calculate_metrics_from_json(json_file_path, tokenizer, model, id2label, device):\n    utterance_metrics = []\n    entity_metrics = {'true_positive': 0, 'false_positive': 0, 'false_negative': 0}\n    total_utterances = 0\n\n    # Load the JSON file\n    with open(json_file_path, 'r') as f:\n        test_data = json.load(f)\n    \n    total_utterances = len(test_data)\n\n    for sample in test_data:\n        # Ground truth entities (set normalization, unordered)\n        ground_truth = {(tag, slot, value) for tag, slot, value, _, _ in sample['span_info']}\n        \n        # Predicted entities (set normalization, unordered)\n        utterance = sample['utterance']\n        predicted_slot_value_pairs = predict_slot_values(utterance)\n        predicted = {(slot.split(':')[0], slot.split(':')[1], value) for slot, value in predicted_slot_value_pairs}\n        \n        # Entity-Level Metrics\n        true_positives = len(ground_truth & predicted)  # Intersection of ground truth and predicted\n        false_positives = len(predicted - ground_truth)  # Predicted but not in ground truth\n        false_negatives = len(ground_truth - predicted)  # Ground truth but not predicted\n\n        entity_metrics['true_positive'] += true_positives\n        entity_metrics['false_positive'] += false_positives\n        entity_metrics['false_negative'] += false_negatives\n\n        # Utterance-Level Metrics (exact match at entity level)\n        precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n        recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n        f1 = (2 * precision * recall / (precision + recall)) if precision + recall > 0 else 0\n\n        utterance_metrics.append({'precision': precision, 'recall': recall, 'f1': f1})\n\n    # Aggregate Utterance-Level Metrics\n    avg_precision = sum(m['precision'] for m in utterance_metrics) / total_utterances\n    avg_recall = sum(m['recall'] for m in utterance_metrics) / total_utterances\n    avg_f1 = sum(m['f1'] for m in utterance_metrics) / total_utterances\n\n    # Aggregate Entity-Level Metrics\n    tp, fp, fn = entity_metrics['true_positive'], entity_metrics['false_positive'], entity_metrics['false_negative']\n    entity_precision = tp / (tp + fp) if tp + fp > 0 else 0\n    entity_recall = tp / (tp + fn) if tp + fn > 0 else 0\n    entity_f1 = (2 * entity_precision * entity_recall / (entity_precision + entity_recall)) if entity_precision + entity_recall > 0 else 0\n\n    # Print Results\n    print(\"Utterance-Level Metrics:\")\n    print(f\"Precision: {avg_precision:.2f}, Recall: {avg_recall:.2f}, F1 Score: {avg_f1:.2f}\")\n    print(\"Entity-Level Metrics:\")\n    print(f\"Precision: {entity_precision:.2f}, Recall: {entity_recall:.2f}, F1 Score: {entity_f1:.2f}\")\n\n# Example usage:\ncalculate_metrics_from_json('/kaggle/working/data/data_slot_filling_test.json', tokenizer, model, id2label, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T14:11:05.963429Z","iopub.execute_input":"2024-12-31T14:11:05.963770Z","iopub.status.idle":"2024-12-31T14:12:28.804467Z","shell.execute_reply.started":"2024-12-31T14:11:05.963741Z","shell.execute_reply":"2024-12-31T14:12:28.803726Z"}},"outputs":[{"name":"stdout","text":"Utterance-Level Metrics:\nPrecision: 0.70, Recall: 0.70, F1 Score: 0.70\nEntity-Level Metrics:\nPrecision: 0.72, Recall: 0.73, F1 Score: 0.72\n","output_type":"stream"}],"execution_count":78}]}